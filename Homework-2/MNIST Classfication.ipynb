{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfMzjl_8GeFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOI-9Rw7T-hn",
        "colab_type": "text"
      },
      "source": [
        "###Import MNIST data from the cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUgazfhiG-XA",
        "colab_type": "code",
        "outputId": "6aee0029-f562-4517-c357-cd34724e95b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
            "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLVsp4IzIRlL",
        "colab_type": "code",
        "outputId": "26fae06a-9711-4c82-88cb-abcd932ac507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print(mnist.train.labels[4])\n",
        "plt.imshow(np.reshape(mnist.train.images[4], (28,28)), cmap='gray')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff7c42a8da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFWX4paIMv1SU4ZeKMvxSUYZfKur/AJ8Z1K07/LPZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEGD5ZF0I3G2",
        "colab_type": "code",
        "outputId": "523a7cca-9e06-4e0c-d463-0c375266c1ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print(mnist.train.labels[8])\n",
        "plt.imshow(np.reshape(mnist.train.images[8], (28,28)), cmap='gray')"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff7c4286898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEX4gKcIPJEX4gaQIP5AU4QeS+j9CsCjUJ8BjhAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpLxUGQQKRG9",
        "colab_type": "code",
        "outputId": "3fc52626-fd96-48c9-90bb-1019eef6b491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(mnist.train.labels.shape)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SsGazlidiRT",
        "colab_type": "code",
        "outputId": "0291dc58-fc4b-4661-97ed-625c376acbf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(mnist.train.labels[0])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thEI7t3sUMpe",
        "colab_type": "text"
      },
      "source": [
        "###Defining input and variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-TKjePsPHJq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2a12ddad-7ebd-4bd7-853e-dc3107ec2003"
      },
      "source": [
        "x = tf.placeholder(\"float\", [None, 784])\n",
        "W = tf.Variable(tf.zeros([784,10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "y_ = tf.placeholder(\"float\", [None, 10])\n",
        "\n",
        "print(\"x: \",x,\"\\nW: \",W,\"\\nb: \",b,\"\\ny_: \",y_)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:  Tensor(\"Placeholder_20:0\", shape=(?, 784), dtype=float32) \n",
            "W:  <tf.Variable 'Variable_20:0' shape=(784, 10) dtype=float32_ref> \n",
            "b:  <tf.Variable 'Variable_21:0' shape=(10,) dtype=float32_ref> \n",
            "y_:  Tensor(\"Placeholder_21:0\", shape=(?, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYo3BBiXUaI_",
        "colab_type": "text"
      },
      "source": [
        "###Neural Network for MNIST classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1bDd-7pPlBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = tf.nn.softmax(tf.matmul(x,W)+b)\n",
        "cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
        "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1zWz87yPp-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-zqv2kFUlBP",
        "colab_type": "text"
      },
      "source": [
        "####Training process using Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHPsgcBAeDuz",
        "colab_type": "code",
        "outputId": "c66917d0-b59e-4a6b-c640-dfec5ade6b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "combined_loss = []\n",
        "for i in range(1000):\n",
        "  batch_xs, batch_ys = mnist.train.next_batch(50)\n",
        "  loss, step = sess.run([cross_entropy, train_step], feed_dict = {x: batch_xs, y_: batch_ys})\n",
        "  combined_loss.append(loss)\n",
        "  print(\"Iteration: \", i, \"|| Loss: \",loss)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  0 || Loss:  115.129265\n",
            "Iteration:  1 || Loss:  112.91498\n",
            "Iteration:  2 || Loss:  110.05109\n",
            "Iteration:  3 || Loss:  107.99462\n",
            "Iteration:  4 || Loss:  105.38079\n",
            "Iteration:  5 || Loss:  106.32762\n",
            "Iteration:  6 || Loss:  100.853355\n",
            "Iteration:  7 || Loss:  99.00484\n",
            "Iteration:  8 || Loss:  97.14795\n",
            "Iteration:  9 || Loss:  97.19629\n",
            "Iteration:  10 || Loss:  90.86159\n",
            "Iteration:  11 || Loss:  94.206116\n",
            "Iteration:  12 || Loss:  96.14871\n",
            "Iteration:  13 || Loss:  89.68089\n",
            "Iteration:  14 || Loss:  88.03455\n",
            "Iteration:  15 || Loss:  82.61012\n",
            "Iteration:  16 || Loss:  85.15178\n",
            "Iteration:  17 || Loss:  77.55069\n",
            "Iteration:  18 || Loss:  79.0556\n",
            "Iteration:  19 || Loss:  83.05857\n",
            "Iteration:  20 || Loss:  76.33248\n",
            "Iteration:  21 || Loss:  80.67259\n",
            "Iteration:  22 || Loss:  75.996155\n",
            "Iteration:  23 || Loss:  69.80636\n",
            "Iteration:  24 || Loss:  69.02705\n",
            "Iteration:  25 || Loss:  74.3072\n",
            "Iteration:  26 || Loss:  76.64535\n",
            "Iteration:  27 || Loss:  73.84583\n",
            "Iteration:  28 || Loss:  70.05492\n",
            "Iteration:  29 || Loss:  67.46995\n",
            "Iteration:  30 || Loss:  64.22806\n",
            "Iteration:  31 || Loss:  66.46435\n",
            "Iteration:  32 || Loss:  66.620094\n",
            "Iteration:  33 || Loss:  68.045105\n",
            "Iteration:  34 || Loss:  66.17473\n",
            "Iteration:  35 || Loss:  66.798485\n",
            "Iteration:  36 || Loss:  64.02363\n",
            "Iteration:  37 || Loss:  64.088\n",
            "Iteration:  38 || Loss:  69.539\n",
            "Iteration:  39 || Loss:  54.4275\n",
            "Iteration:  40 || Loss:  56.29235\n",
            "Iteration:  41 || Loss:  63.748333\n",
            "Iteration:  42 || Loss:  59.1027\n",
            "Iteration:  43 || Loss:  63.7517\n",
            "Iteration:  44 || Loss:  58.577774\n",
            "Iteration:  45 || Loss:  59.692223\n",
            "Iteration:  46 || Loss:  58.37369\n",
            "Iteration:  47 || Loss:  54.80437\n",
            "Iteration:  48 || Loss:  59.86475\n",
            "Iteration:  49 || Loss:  58.431587\n",
            "Iteration:  50 || Loss:  59.945503\n",
            "Iteration:  51 || Loss:  51.493927\n",
            "Iteration:  52 || Loss:  56.1924\n",
            "Iteration:  53 || Loss:  53.86587\n",
            "Iteration:  54 || Loss:  50.960358\n",
            "Iteration:  55 || Loss:  55.083736\n",
            "Iteration:  56 || Loss:  47.267853\n",
            "Iteration:  57 || Loss:  48.548664\n",
            "Iteration:  58 || Loss:  50.341843\n",
            "Iteration:  59 || Loss:  57.51899\n",
            "Iteration:  60 || Loss:  50.845192\n",
            "Iteration:  61 || Loss:  54.511143\n",
            "Iteration:  62 || Loss:  47.0384\n",
            "Iteration:  63 || Loss:  56.892803\n",
            "Iteration:  64 || Loss:  42.5545\n",
            "Iteration:  65 || Loss:  48.8723\n",
            "Iteration:  66 || Loss:  49.950333\n",
            "Iteration:  67 || Loss:  46.538017\n",
            "Iteration:  68 || Loss:  41.624454\n",
            "Iteration:  69 || Loss:  55.349415\n",
            "Iteration:  70 || Loss:  51.85389\n",
            "Iteration:  71 || Loss:  51.27375\n",
            "Iteration:  72 || Loss:  39.9579\n",
            "Iteration:  73 || Loss:  52.956566\n",
            "Iteration:  74 || Loss:  41.683517\n",
            "Iteration:  75 || Loss:  43.603924\n",
            "Iteration:  76 || Loss:  44.494278\n",
            "Iteration:  77 || Loss:  47.373226\n",
            "Iteration:  78 || Loss:  49.44353\n",
            "Iteration:  79 || Loss:  45.02879\n",
            "Iteration:  80 || Loss:  45.610394\n",
            "Iteration:  81 || Loss:  47.412548\n",
            "Iteration:  82 || Loss:  48.76187\n",
            "Iteration:  83 || Loss:  41.359562\n",
            "Iteration:  84 || Loss:  41.660885\n",
            "Iteration:  85 || Loss:  38.94014\n",
            "Iteration:  86 || Loss:  39.187393\n",
            "Iteration:  87 || Loss:  46.487904\n",
            "Iteration:  88 || Loss:  35.023964\n",
            "Iteration:  89 || Loss:  50.474037\n",
            "Iteration:  90 || Loss:  40.663147\n",
            "Iteration:  91 || Loss:  36.535637\n",
            "Iteration:  92 || Loss:  47.19012\n",
            "Iteration:  93 || Loss:  36.288307\n",
            "Iteration:  94 || Loss:  41.714897\n",
            "Iteration:  95 || Loss:  39.261517\n",
            "Iteration:  96 || Loss:  47.799747\n",
            "Iteration:  97 || Loss:  37.439896\n",
            "Iteration:  98 || Loss:  41.345333\n",
            "Iteration:  99 || Loss:  39.456974\n",
            "Iteration:  100 || Loss:  41.057434\n",
            "Iteration:  101 || Loss:  40.342003\n",
            "Iteration:  102 || Loss:  46.82446\n",
            "Iteration:  103 || Loss:  38.79979\n",
            "Iteration:  104 || Loss:  36.724037\n",
            "Iteration:  105 || Loss:  46.49758\n",
            "Iteration:  106 || Loss:  39.248394\n",
            "Iteration:  107 || Loss:  42.88679\n",
            "Iteration:  108 || Loss:  35.41645\n",
            "Iteration:  109 || Loss:  36.540108\n",
            "Iteration:  110 || Loss:  38.16747\n",
            "Iteration:  111 || Loss:  30.43251\n",
            "Iteration:  112 || Loss:  37.648945\n",
            "Iteration:  113 || Loss:  33.58898\n",
            "Iteration:  114 || Loss:  36.484985\n",
            "Iteration:  115 || Loss:  42.99073\n",
            "Iteration:  116 || Loss:  46.89654\n",
            "Iteration:  117 || Loss:  41.325226\n",
            "Iteration:  118 || Loss:  38.881134\n",
            "Iteration:  119 || Loss:  31.705673\n",
            "Iteration:  120 || Loss:  33.17802\n",
            "Iteration:  121 || Loss:  35.722836\n",
            "Iteration:  122 || Loss:  38.979057\n",
            "Iteration:  123 || Loss:  34.154476\n",
            "Iteration:  124 || Loss:  43.796516\n",
            "Iteration:  125 || Loss:  39.73386\n",
            "Iteration:  126 || Loss:  33.093895\n",
            "Iteration:  127 || Loss:  36.96063\n",
            "Iteration:  128 || Loss:  44.422165\n",
            "Iteration:  129 || Loss:  30.649439\n",
            "Iteration:  130 || Loss:  29.213308\n",
            "Iteration:  131 || Loss:  39.385113\n",
            "Iteration:  132 || Loss:  30.401367\n",
            "Iteration:  133 || Loss:  42.727463\n",
            "Iteration:  134 || Loss:  36.101387\n",
            "Iteration:  135 || Loss:  35.127644\n",
            "Iteration:  136 || Loss:  35.702103\n",
            "Iteration:  137 || Loss:  32.501747\n",
            "Iteration:  138 || Loss:  35.911407\n",
            "Iteration:  139 || Loss:  49.43677\n",
            "Iteration:  140 || Loss:  28.778372\n",
            "Iteration:  141 || Loss:  35.19289\n",
            "Iteration:  142 || Loss:  31.489029\n",
            "Iteration:  143 || Loss:  41.983814\n",
            "Iteration:  144 || Loss:  32.418446\n",
            "Iteration:  145 || Loss:  32.55381\n",
            "Iteration:  146 || Loss:  29.647768\n",
            "Iteration:  147 || Loss:  28.65317\n",
            "Iteration:  148 || Loss:  36.689877\n",
            "Iteration:  149 || Loss:  30.523342\n",
            "Iteration:  150 || Loss:  34.16625\n",
            "Iteration:  151 || Loss:  30.190216\n",
            "Iteration:  152 || Loss:  35.57964\n",
            "Iteration:  153 || Loss:  36.23697\n",
            "Iteration:  154 || Loss:  34.582436\n",
            "Iteration:  155 || Loss:  29.245247\n",
            "Iteration:  156 || Loss:  36.083324\n",
            "Iteration:  157 || Loss:  38.426067\n",
            "Iteration:  158 || Loss:  36.238754\n",
            "Iteration:  159 || Loss:  24.34003\n",
            "Iteration:  160 || Loss:  45.011024\n",
            "Iteration:  161 || Loss:  26.112455\n",
            "Iteration:  162 || Loss:  36.932922\n",
            "Iteration:  163 || Loss:  32.61814\n",
            "Iteration:  164 || Loss:  33.167755\n",
            "Iteration:  165 || Loss:  39.18764\n",
            "Iteration:  166 || Loss:  27.446978\n",
            "Iteration:  167 || Loss:  34.551804\n",
            "Iteration:  168 || Loss:  39.11473\n",
            "Iteration:  169 || Loss:  33.40287\n",
            "Iteration:  170 || Loss:  26.250118\n",
            "Iteration:  171 || Loss:  26.208387\n",
            "Iteration:  172 || Loss:  31.162325\n",
            "Iteration:  173 || Loss:  32.370262\n",
            "Iteration:  174 || Loss:  28.74656\n",
            "Iteration:  175 || Loss:  30.20413\n",
            "Iteration:  176 || Loss:  35.77037\n",
            "Iteration:  177 || Loss:  47.382175\n",
            "Iteration:  178 || Loss:  36.579464\n",
            "Iteration:  179 || Loss:  32.885838\n",
            "Iteration:  180 || Loss:  30.077343\n",
            "Iteration:  181 || Loss:  28.971893\n",
            "Iteration:  182 || Loss:  27.53903\n",
            "Iteration:  183 || Loss:  33.944942\n",
            "Iteration:  184 || Loss:  31.023441\n",
            "Iteration:  185 || Loss:  24.840046\n",
            "Iteration:  186 || Loss:  36.129047\n",
            "Iteration:  187 || Loss:  20.485636\n",
            "Iteration:  188 || Loss:  34.948452\n",
            "Iteration:  189 || Loss:  29.96681\n",
            "Iteration:  190 || Loss:  30.061491\n",
            "Iteration:  191 || Loss:  31.909431\n",
            "Iteration:  192 || Loss:  25.589823\n",
            "Iteration:  193 || Loss:  29.947731\n",
            "Iteration:  194 || Loss:  29.390469\n",
            "Iteration:  195 || Loss:  26.033049\n",
            "Iteration:  196 || Loss:  21.8325\n",
            "Iteration:  197 || Loss:  26.45311\n",
            "Iteration:  198 || Loss:  33.040443\n",
            "Iteration:  199 || Loss:  30.35416\n",
            "Iteration:  200 || Loss:  38.846962\n",
            "Iteration:  201 || Loss:  31.73058\n",
            "Iteration:  202 || Loss:  26.67638\n",
            "Iteration:  203 || Loss:  29.627552\n",
            "Iteration:  204 || Loss:  30.46259\n",
            "Iteration:  205 || Loss:  43.830074\n",
            "Iteration:  206 || Loss:  37.52704\n",
            "Iteration:  207 || Loss:  28.89276\n",
            "Iteration:  208 || Loss:  27.121067\n",
            "Iteration:  209 || Loss:  28.772392\n",
            "Iteration:  210 || Loss:  33.427563\n",
            "Iteration:  211 || Loss:  26.462128\n",
            "Iteration:  212 || Loss:  22.944107\n",
            "Iteration:  213 || Loss:  34.880188\n",
            "Iteration:  214 || Loss:  37.540318\n",
            "Iteration:  215 || Loss:  29.215925\n",
            "Iteration:  216 || Loss:  34.419945\n",
            "Iteration:  217 || Loss:  27.8731\n",
            "Iteration:  218 || Loss:  28.08442\n",
            "Iteration:  219 || Loss:  30.20538\n",
            "Iteration:  220 || Loss:  33.385704\n",
            "Iteration:  221 || Loss:  27.445345\n",
            "Iteration:  222 || Loss:  25.251467\n",
            "Iteration:  223 || Loss:  23.49291\n",
            "Iteration:  224 || Loss:  29.672562\n",
            "Iteration:  225 || Loss:  23.56457\n",
            "Iteration:  226 || Loss:  29.125168\n",
            "Iteration:  227 || Loss:  34.7808\n",
            "Iteration:  228 || Loss:  21.970673\n",
            "Iteration:  229 || Loss:  25.993176\n",
            "Iteration:  230 || Loss:  25.415808\n",
            "Iteration:  231 || Loss:  31.138521\n",
            "Iteration:  232 || Loss:  35.622482\n",
            "Iteration:  233 || Loss:  25.83177\n",
            "Iteration:  234 || Loss:  32.02552\n",
            "Iteration:  235 || Loss:  23.364525\n",
            "Iteration:  236 || Loss:  36.208515\n",
            "Iteration:  237 || Loss:  32.848064\n",
            "Iteration:  238 || Loss:  29.396025\n",
            "Iteration:  239 || Loss:  27.195187\n",
            "Iteration:  240 || Loss:  21.93404\n",
            "Iteration:  241 || Loss:  30.138132\n",
            "Iteration:  242 || Loss:  38.50564\n",
            "Iteration:  243 || Loss:  27.802467\n",
            "Iteration:  244 || Loss:  27.991726\n",
            "Iteration:  245 || Loss:  26.662615\n",
            "Iteration:  246 || Loss:  29.46621\n",
            "Iteration:  247 || Loss:  22.895939\n",
            "Iteration:  248 || Loss:  30.38142\n",
            "Iteration:  249 || Loss:  27.553875\n",
            "Iteration:  250 || Loss:  29.599907\n",
            "Iteration:  251 || Loss:  33.479366\n",
            "Iteration:  252 || Loss:  34.04816\n",
            "Iteration:  253 || Loss:  21.042837\n",
            "Iteration:  254 || Loss:  24.489172\n",
            "Iteration:  255 || Loss:  25.112844\n",
            "Iteration:  256 || Loss:  27.19451\n",
            "Iteration:  257 || Loss:  25.601038\n",
            "Iteration:  258 || Loss:  26.508783\n",
            "Iteration:  259 || Loss:  22.834606\n",
            "Iteration:  260 || Loss:  27.339111\n",
            "Iteration:  261 || Loss:  31.217491\n",
            "Iteration:  262 || Loss:  30.118076\n",
            "Iteration:  263 || Loss:  24.61218\n",
            "Iteration:  264 || Loss:  24.635176\n",
            "Iteration:  265 || Loss:  26.242992\n",
            "Iteration:  266 || Loss:  36.76613\n",
            "Iteration:  267 || Loss:  27.525713\n",
            "Iteration:  268 || Loss:  36.65963\n",
            "Iteration:  269 || Loss:  32.0254\n",
            "Iteration:  270 || Loss:  29.68559\n",
            "Iteration:  271 || Loss:  32.953445\n",
            "Iteration:  272 || Loss:  21.203535\n",
            "Iteration:  273 || Loss:  30.012033\n",
            "Iteration:  274 || Loss:  35.404358\n",
            "Iteration:  275 || Loss:  30.938688\n",
            "Iteration:  276 || Loss:  24.822037\n",
            "Iteration:  277 || Loss:  30.071032\n",
            "Iteration:  278 || Loss:  22.732235\n",
            "Iteration:  279 || Loss:  27.540867\n",
            "Iteration:  280 || Loss:  28.862186\n",
            "Iteration:  281 || Loss:  23.874268\n",
            "Iteration:  282 || Loss:  20.102585\n",
            "Iteration:  283 || Loss:  28.399372\n",
            "Iteration:  284 || Loss:  28.578032\n",
            "Iteration:  285 || Loss:  29.307177\n",
            "Iteration:  286 || Loss:  26.851177\n",
            "Iteration:  287 || Loss:  29.992579\n",
            "Iteration:  288 || Loss:  26.157719\n",
            "Iteration:  289 || Loss:  22.832125\n",
            "Iteration:  290 || Loss:  22.10226\n",
            "Iteration:  291 || Loss:  27.45787\n",
            "Iteration:  292 || Loss:  31.706917\n",
            "Iteration:  293 || Loss:  29.506308\n",
            "Iteration:  294 || Loss:  25.766312\n",
            "Iteration:  295 || Loss:  23.7225\n",
            "Iteration:  296 || Loss:  28.0523\n",
            "Iteration:  297 || Loss:  25.122673\n",
            "Iteration:  298 || Loss:  28.845345\n",
            "Iteration:  299 || Loss:  29.649195\n",
            "Iteration:  300 || Loss:  34.06716\n",
            "Iteration:  301 || Loss:  36.191242\n",
            "Iteration:  302 || Loss:  25.617641\n",
            "Iteration:  303 || Loss:  31.265362\n",
            "Iteration:  304 || Loss:  26.761251\n",
            "Iteration:  305 || Loss:  28.649387\n",
            "Iteration:  306 || Loss:  27.796406\n",
            "Iteration:  307 || Loss:  25.565401\n",
            "Iteration:  308 || Loss:  36.667393\n",
            "Iteration:  309 || Loss:  22.59722\n",
            "Iteration:  310 || Loss:  33.27717\n",
            "Iteration:  311 || Loss:  28.630997\n",
            "Iteration:  312 || Loss:  20.699547\n",
            "Iteration:  313 || Loss:  25.575747\n",
            "Iteration:  314 || Loss:  29.08785\n",
            "Iteration:  315 || Loss:  22.99018\n",
            "Iteration:  316 || Loss:  25.967304\n",
            "Iteration:  317 || Loss:  27.950987\n",
            "Iteration:  318 || Loss:  30.071533\n",
            "Iteration:  319 || Loss:  23.361568\n",
            "Iteration:  320 || Loss:  32.03149\n",
            "Iteration:  321 || Loss:  30.511019\n",
            "Iteration:  322 || Loss:  34.303688\n",
            "Iteration:  323 || Loss:  29.132828\n",
            "Iteration:  324 || Loss:  32.635082\n",
            "Iteration:  325 || Loss:  32.00229\n",
            "Iteration:  326 || Loss:  30.365583\n",
            "Iteration:  327 || Loss:  29.593796\n",
            "Iteration:  328 || Loss:  25.481806\n",
            "Iteration:  329 || Loss:  20.751108\n",
            "Iteration:  330 || Loss:  29.364368\n",
            "Iteration:  331 || Loss:  25.621523\n",
            "Iteration:  332 || Loss:  29.963104\n",
            "Iteration:  333 || Loss:  25.513176\n",
            "Iteration:  334 || Loss:  29.86899\n",
            "Iteration:  335 || Loss:  27.66108\n",
            "Iteration:  336 || Loss:  29.445066\n",
            "Iteration:  337 || Loss:  21.019087\n",
            "Iteration:  338 || Loss:  26.269121\n",
            "Iteration:  339 || Loss:  29.69094\n",
            "Iteration:  340 || Loss:  23.408684\n",
            "Iteration:  341 || Loss:  25.307436\n",
            "Iteration:  342 || Loss:  30.03493\n",
            "Iteration:  343 || Loss:  23.743721\n",
            "Iteration:  344 || Loss:  24.855389\n",
            "Iteration:  345 || Loss:  34.658367\n",
            "Iteration:  346 || Loss:  23.871395\n",
            "Iteration:  347 || Loss:  28.4472\n",
            "Iteration:  348 || Loss:  33.679184\n",
            "Iteration:  349 || Loss:  24.658085\n",
            "Iteration:  350 || Loss:  25.874363\n",
            "Iteration:  351 || Loss:  27.524483\n",
            "Iteration:  352 || Loss:  19.779858\n",
            "Iteration:  353 || Loss:  15.32563\n",
            "Iteration:  354 || Loss:  17.307608\n",
            "Iteration:  355 || Loss:  22.138535\n",
            "Iteration:  356 || Loss:  28.643518\n",
            "Iteration:  357 || Loss:  24.265484\n",
            "Iteration:  358 || Loss:  21.441935\n",
            "Iteration:  359 || Loss:  37.449734\n",
            "Iteration:  360 || Loss:  22.426382\n",
            "Iteration:  361 || Loss:  17.776129\n",
            "Iteration:  362 || Loss:  26.872147\n",
            "Iteration:  363 || Loss:  34.110085\n",
            "Iteration:  364 || Loss:  36.683506\n",
            "Iteration:  365 || Loss:  26.207632\n",
            "Iteration:  366 || Loss:  26.74583\n",
            "Iteration:  367 || Loss:  31.262959\n",
            "Iteration:  368 || Loss:  29.539644\n",
            "Iteration:  369 || Loss:  27.253115\n",
            "Iteration:  370 || Loss:  21.133846\n",
            "Iteration:  371 || Loss:  35.74901\n",
            "Iteration:  372 || Loss:  23.536379\n",
            "Iteration:  373 || Loss:  32.6808\n",
            "Iteration:  374 || Loss:  26.014713\n",
            "Iteration:  375 || Loss:  20.917337\n",
            "Iteration:  376 || Loss:  26.502218\n",
            "Iteration:  377 || Loss:  24.387012\n",
            "Iteration:  378 || Loss:  19.749207\n",
            "Iteration:  379 || Loss:  22.101604\n",
            "Iteration:  380 || Loss:  23.73152\n",
            "Iteration:  381 || Loss:  34.460102\n",
            "Iteration:  382 || Loss:  21.458632\n",
            "Iteration:  383 || Loss:  23.172813\n",
            "Iteration:  384 || Loss:  25.73256\n",
            "Iteration:  385 || Loss:  19.897387\n",
            "Iteration:  386 || Loss:  22.65224\n",
            "Iteration:  387 || Loss:  21.954435\n",
            "Iteration:  388 || Loss:  23.083542\n",
            "Iteration:  389 || Loss:  29.924585\n",
            "Iteration:  390 || Loss:  21.380308\n",
            "Iteration:  391 || Loss:  14.546949\n",
            "Iteration:  392 || Loss:  20.662449\n",
            "Iteration:  393 || Loss:  20.80073\n",
            "Iteration:  394 || Loss:  21.443985\n",
            "Iteration:  395 || Loss:  20.754528\n",
            "Iteration:  396 || Loss:  22.963497\n",
            "Iteration:  397 || Loss:  24.465466\n",
            "Iteration:  398 || Loss:  21.418507\n",
            "Iteration:  399 || Loss:  23.419128\n",
            "Iteration:  400 || Loss:  17.90762\n",
            "Iteration:  401 || Loss:  33.04893\n",
            "Iteration:  402 || Loss:  22.785618\n",
            "Iteration:  403 || Loss:  17.87011\n",
            "Iteration:  404 || Loss:  29.190033\n",
            "Iteration:  405 || Loss:  28.159624\n",
            "Iteration:  406 || Loss:  20.868048\n",
            "Iteration:  407 || Loss:  26.38089\n",
            "Iteration:  408 || Loss:  23.56519\n",
            "Iteration:  409 || Loss:  21.127533\n",
            "Iteration:  410 || Loss:  23.941778\n",
            "Iteration:  411 || Loss:  20.92541\n",
            "Iteration:  412 || Loss:  27.279284\n",
            "Iteration:  413 || Loss:  18.527582\n",
            "Iteration:  414 || Loss:  21.919462\n",
            "Iteration:  415 || Loss:  19.136345\n",
            "Iteration:  416 || Loss:  17.35951\n",
            "Iteration:  417 || Loss:  29.42035\n",
            "Iteration:  418 || Loss:  30.384258\n",
            "Iteration:  419 || Loss:  23.98711\n",
            "Iteration:  420 || Loss:  16.909838\n",
            "Iteration:  421 || Loss:  22.176296\n",
            "Iteration:  422 || Loss:  32.684425\n",
            "Iteration:  423 || Loss:  27.493557\n",
            "Iteration:  424 || Loss:  23.743462\n",
            "Iteration:  425 || Loss:  21.213104\n",
            "Iteration:  426 || Loss:  19.667698\n",
            "Iteration:  427 || Loss:  22.664463\n",
            "Iteration:  428 || Loss:  23.247671\n",
            "Iteration:  429 || Loss:  26.285053\n",
            "Iteration:  430 || Loss:  19.61605\n",
            "Iteration:  431 || Loss:  22.277096\n",
            "Iteration:  432 || Loss:  22.723108\n",
            "Iteration:  433 || Loss:  28.60914\n",
            "Iteration:  434 || Loss:  24.814198\n",
            "Iteration:  435 || Loss:  30.560837\n",
            "Iteration:  436 || Loss:  26.270782\n",
            "Iteration:  437 || Loss:  20.587725\n",
            "Iteration:  438 || Loss:  24.04261\n",
            "Iteration:  439 || Loss:  33.428696\n",
            "Iteration:  440 || Loss:  34.96773\n",
            "Iteration:  441 || Loss:  29.422445\n",
            "Iteration:  442 || Loss:  32.0486\n",
            "Iteration:  443 || Loss:  28.983412\n",
            "Iteration:  444 || Loss:  20.401485\n",
            "Iteration:  445 || Loss:  23.521935\n",
            "Iteration:  446 || Loss:  23.863379\n",
            "Iteration:  447 || Loss:  17.585297\n",
            "Iteration:  448 || Loss:  26.332855\n",
            "Iteration:  449 || Loss:  16.579767\n",
            "Iteration:  450 || Loss:  17.330364\n",
            "Iteration:  451 || Loss:  15.369259\n",
            "Iteration:  452 || Loss:  28.681005\n",
            "Iteration:  453 || Loss:  19.2255\n",
            "Iteration:  454 || Loss:  22.415512\n",
            "Iteration:  455 || Loss:  29.111021\n",
            "Iteration:  456 || Loss:  26.427885\n",
            "Iteration:  457 || Loss:  29.54146\n",
            "Iteration:  458 || Loss:  18.90925\n",
            "Iteration:  459 || Loss:  25.68182\n",
            "Iteration:  460 || Loss:  30.346071\n",
            "Iteration:  461 || Loss:  24.910608\n",
            "Iteration:  462 || Loss:  38.154736\n",
            "Iteration:  463 || Loss:  31.536472\n",
            "Iteration:  464 || Loss:  25.601576\n",
            "Iteration:  465 || Loss:  18.13458\n",
            "Iteration:  466 || Loss:  30.416164\n",
            "Iteration:  467 || Loss:  19.92284\n",
            "Iteration:  468 || Loss:  21.924372\n",
            "Iteration:  469 || Loss:  23.19842\n",
            "Iteration:  470 || Loss:  24.319109\n",
            "Iteration:  471 || Loss:  22.594646\n",
            "Iteration:  472 || Loss:  20.746569\n",
            "Iteration:  473 || Loss:  15.658131\n",
            "Iteration:  474 || Loss:  23.837013\n",
            "Iteration:  475 || Loss:  20.317144\n",
            "Iteration:  476 || Loss:  17.928112\n",
            "Iteration:  477 || Loss:  35.519035\n",
            "Iteration:  478 || Loss:  17.66866\n",
            "Iteration:  479 || Loss:  28.650166\n",
            "Iteration:  480 || Loss:  12.3478\n",
            "Iteration:  481 || Loss:  35.935326\n",
            "Iteration:  482 || Loss:  15.708634\n",
            "Iteration:  483 || Loss:  21.016174\n",
            "Iteration:  484 || Loss:  21.2687\n",
            "Iteration:  485 || Loss:  24.291977\n",
            "Iteration:  486 || Loss:  24.968632\n",
            "Iteration:  487 || Loss:  14.571335\n",
            "Iteration:  488 || Loss:  19.128492\n",
            "Iteration:  489 || Loss:  29.564522\n",
            "Iteration:  490 || Loss:  27.850159\n",
            "Iteration:  491 || Loss:  21.804985\n",
            "Iteration:  492 || Loss:  19.115097\n",
            "Iteration:  493 || Loss:  19.013847\n",
            "Iteration:  494 || Loss:  23.866407\n",
            "Iteration:  495 || Loss:  20.556038\n",
            "Iteration:  496 || Loss:  21.27542\n",
            "Iteration:  497 || Loss:  21.173744\n",
            "Iteration:  498 || Loss:  25.715675\n",
            "Iteration:  499 || Loss:  16.877539\n",
            "Iteration:  500 || Loss:  26.020432\n",
            "Iteration:  501 || Loss:  18.985067\n",
            "Iteration:  502 || Loss:  15.784861\n",
            "Iteration:  503 || Loss:  24.892487\n",
            "Iteration:  504 || Loss:  17.263895\n",
            "Iteration:  505 || Loss:  23.534847\n",
            "Iteration:  506 || Loss:  24.827929\n",
            "Iteration:  507 || Loss:  31.447304\n",
            "Iteration:  508 || Loss:  14.905913\n",
            "Iteration:  509 || Loss:  19.958866\n",
            "Iteration:  510 || Loss:  24.127604\n",
            "Iteration:  511 || Loss:  20.945723\n",
            "Iteration:  512 || Loss:  20.654306\n",
            "Iteration:  513 || Loss:  22.947166\n",
            "Iteration:  514 || Loss:  15.733316\n",
            "Iteration:  515 || Loss:  25.760317\n",
            "Iteration:  516 || Loss:  21.652714\n",
            "Iteration:  517 || Loss:  29.455193\n",
            "Iteration:  518 || Loss:  18.453363\n",
            "Iteration:  519 || Loss:  25.521929\n",
            "Iteration:  520 || Loss:  24.02736\n",
            "Iteration:  521 || Loss:  16.548836\n",
            "Iteration:  522 || Loss:  18.435095\n",
            "Iteration:  523 || Loss:  19.204916\n",
            "Iteration:  524 || Loss:  26.688957\n",
            "Iteration:  525 || Loss:  23.681503\n",
            "Iteration:  526 || Loss:  16.438799\n",
            "Iteration:  527 || Loss:  27.192911\n",
            "Iteration:  528 || Loss:  23.325554\n",
            "Iteration:  529 || Loss:  20.426876\n",
            "Iteration:  530 || Loss:  23.521503\n",
            "Iteration:  531 || Loss:  26.36054\n",
            "Iteration:  532 || Loss:  35.10877\n",
            "Iteration:  533 || Loss:  31.004744\n",
            "Iteration:  534 || Loss:  16.426735\n",
            "Iteration:  535 || Loss:  31.607506\n",
            "Iteration:  536 || Loss:  19.255384\n",
            "Iteration:  537 || Loss:  15.038088\n",
            "Iteration:  538 || Loss:  22.637392\n",
            "Iteration:  539 || Loss:  17.571196\n",
            "Iteration:  540 || Loss:  21.07032\n",
            "Iteration:  541 || Loss:  22.043869\n",
            "Iteration:  542 || Loss:  34.109497\n",
            "Iteration:  543 || Loss:  23.145521\n",
            "Iteration:  544 || Loss:  25.699583\n",
            "Iteration:  545 || Loss:  31.084837\n",
            "Iteration:  546 || Loss:  20.713892\n",
            "Iteration:  547 || Loss:  19.072506\n",
            "Iteration:  548 || Loss:  12.675177\n",
            "Iteration:  549 || Loss:  14.317123\n",
            "Iteration:  550 || Loss:  24.739601\n",
            "Iteration:  551 || Loss:  20.464231\n",
            "Iteration:  552 || Loss:  18.232695\n",
            "Iteration:  553 || Loss:  29.784071\n",
            "Iteration:  554 || Loss:  32.761898\n",
            "Iteration:  555 || Loss:  12.464661\n",
            "Iteration:  556 || Loss:  23.692394\n",
            "Iteration:  557 || Loss:  16.556925\n",
            "Iteration:  558 || Loss:  26.489548\n",
            "Iteration:  559 || Loss:  23.763123\n",
            "Iteration:  560 || Loss:  31.679995\n",
            "Iteration:  561 || Loss:  25.31739\n",
            "Iteration:  562 || Loss:  31.593502\n",
            "Iteration:  563 || Loss:  25.623888\n",
            "Iteration:  564 || Loss:  21.825315\n",
            "Iteration:  565 || Loss:  17.920456\n",
            "Iteration:  566 || Loss:  27.915445\n",
            "Iteration:  567 || Loss:  21.27213\n",
            "Iteration:  568 || Loss:  22.285088\n",
            "Iteration:  569 || Loss:  26.640135\n",
            "Iteration:  570 || Loss:  21.022423\n",
            "Iteration:  571 || Loss:  20.493832\n",
            "Iteration:  572 || Loss:  14.919028\n",
            "Iteration:  573 || Loss:  18.073767\n",
            "Iteration:  574 || Loss:  23.623302\n",
            "Iteration:  575 || Loss:  17.306423\n",
            "Iteration:  576 || Loss:  20.071484\n",
            "Iteration:  577 || Loss:  31.787252\n",
            "Iteration:  578 || Loss:  17.783869\n",
            "Iteration:  579 || Loss:  25.976416\n",
            "Iteration:  580 || Loss:  29.731209\n",
            "Iteration:  581 || Loss:  20.385704\n",
            "Iteration:  582 || Loss:  24.276741\n",
            "Iteration:  583 || Loss:  22.160606\n",
            "Iteration:  584 || Loss:  20.124561\n",
            "Iteration:  585 || Loss:  26.97787\n",
            "Iteration:  586 || Loss:  22.235472\n",
            "Iteration:  587 || Loss:  24.456295\n",
            "Iteration:  588 || Loss:  16.447887\n",
            "Iteration:  589 || Loss:  23.643295\n",
            "Iteration:  590 || Loss:  18.596857\n",
            "Iteration:  591 || Loss:  26.505117\n",
            "Iteration:  592 || Loss:  24.495054\n",
            "Iteration:  593 || Loss:  22.09136\n",
            "Iteration:  594 || Loss:  20.317574\n",
            "Iteration:  595 || Loss:  19.418\n",
            "Iteration:  596 || Loss:  20.237036\n",
            "Iteration:  597 || Loss:  21.931862\n",
            "Iteration:  598 || Loss:  26.44825\n",
            "Iteration:  599 || Loss:  21.078413\n",
            "Iteration:  600 || Loss:  26.185753\n",
            "Iteration:  601 || Loss:  21.200672\n",
            "Iteration:  602 || Loss:  22.162018\n",
            "Iteration:  603 || Loss:  28.283867\n",
            "Iteration:  604 || Loss:  16.016108\n",
            "Iteration:  605 || Loss:  15.462439\n",
            "Iteration:  606 || Loss:  16.739265\n",
            "Iteration:  607 || Loss:  22.088112\n",
            "Iteration:  608 || Loss:  16.676216\n",
            "Iteration:  609 || Loss:  28.60902\n",
            "Iteration:  610 || Loss:  21.96683\n",
            "Iteration:  611 || Loss:  18.59781\n",
            "Iteration:  612 || Loss:  25.581976\n",
            "Iteration:  613 || Loss:  28.527657\n",
            "Iteration:  614 || Loss:  26.395166\n",
            "Iteration:  615 || Loss:  23.820465\n",
            "Iteration:  616 || Loss:  16.395927\n",
            "Iteration:  617 || Loss:  27.059029\n",
            "Iteration:  618 || Loss:  17.103357\n",
            "Iteration:  619 || Loss:  23.785805\n",
            "Iteration:  620 || Loss:  31.321712\n",
            "Iteration:  621 || Loss:  19.39217\n",
            "Iteration:  622 || Loss:  26.68821\n",
            "Iteration:  623 || Loss:  17.057882\n",
            "Iteration:  624 || Loss:  20.287588\n",
            "Iteration:  625 || Loss:  23.569351\n",
            "Iteration:  626 || Loss:  27.799404\n",
            "Iteration:  627 || Loss:  23.824408\n",
            "Iteration:  628 || Loss:  27.070692\n",
            "Iteration:  629 || Loss:  23.999277\n",
            "Iteration:  630 || Loss:  18.195894\n",
            "Iteration:  631 || Loss:  21.518284\n",
            "Iteration:  632 || Loss:  22.629675\n",
            "Iteration:  633 || Loss:  18.208824\n",
            "Iteration:  634 || Loss:  29.76971\n",
            "Iteration:  635 || Loss:  21.373465\n",
            "Iteration:  636 || Loss:  19.950056\n",
            "Iteration:  637 || Loss:  20.648588\n",
            "Iteration:  638 || Loss:  12.509519\n",
            "Iteration:  639 || Loss:  22.079954\n",
            "Iteration:  640 || Loss:  24.194437\n",
            "Iteration:  641 || Loss:  9.180121\n",
            "Iteration:  642 || Loss:  23.53849\n",
            "Iteration:  643 || Loss:  29.63344\n",
            "Iteration:  644 || Loss:  18.490646\n",
            "Iteration:  645 || Loss:  21.360489\n",
            "Iteration:  646 || Loss:  27.6083\n",
            "Iteration:  647 || Loss:  13.015912\n",
            "Iteration:  648 || Loss:  23.119541\n",
            "Iteration:  649 || Loss:  15.005353\n",
            "Iteration:  650 || Loss:  24.35541\n",
            "Iteration:  651 || Loss:  17.579468\n",
            "Iteration:  652 || Loss:  12.378235\n",
            "Iteration:  653 || Loss:  29.731228\n",
            "Iteration:  654 || Loss:  21.701447\n",
            "Iteration:  655 || Loss:  29.54519\n",
            "Iteration:  656 || Loss:  26.816864\n",
            "Iteration:  657 || Loss:  28.13147\n",
            "Iteration:  658 || Loss:  25.848038\n",
            "Iteration:  659 || Loss:  21.850735\n",
            "Iteration:  660 || Loss:  29.734846\n",
            "Iteration:  661 || Loss:  12.291902\n",
            "Iteration:  662 || Loss:  13.451874\n",
            "Iteration:  663 || Loss:  19.328016\n",
            "Iteration:  664 || Loss:  20.990566\n",
            "Iteration:  665 || Loss:  17.770912\n",
            "Iteration:  666 || Loss:  31.216005\n",
            "Iteration:  667 || Loss:  18.207735\n",
            "Iteration:  668 || Loss:  26.783283\n",
            "Iteration:  669 || Loss:  18.505754\n",
            "Iteration:  670 || Loss:  17.925163\n",
            "Iteration:  671 || Loss:  12.958388\n",
            "Iteration:  672 || Loss:  22.595634\n",
            "Iteration:  673 || Loss:  32.835796\n",
            "Iteration:  674 || Loss:  17.718287\n",
            "Iteration:  675 || Loss:  23.537315\n",
            "Iteration:  676 || Loss:  21.03645\n",
            "Iteration:  677 || Loss:  22.415173\n",
            "Iteration:  678 || Loss:  22.57313\n",
            "Iteration:  679 || Loss:  26.42621\n",
            "Iteration:  680 || Loss:  25.576813\n",
            "Iteration:  681 || Loss:  24.009039\n",
            "Iteration:  682 || Loss:  18.199564\n",
            "Iteration:  683 || Loss:  19.981293\n",
            "Iteration:  684 || Loss:  26.26075\n",
            "Iteration:  685 || Loss:  20.622398\n",
            "Iteration:  686 || Loss:  22.068113\n",
            "Iteration:  687 || Loss:  20.96239\n",
            "Iteration:  688 || Loss:  16.52302\n",
            "Iteration:  689 || Loss:  16.417189\n",
            "Iteration:  690 || Loss:  26.393847\n",
            "Iteration:  691 || Loss:  18.694723\n",
            "Iteration:  692 || Loss:  26.385681\n",
            "Iteration:  693 || Loss:  21.907866\n",
            "Iteration:  694 || Loss:  16.634733\n",
            "Iteration:  695 || Loss:  21.423134\n",
            "Iteration:  696 || Loss:  22.714035\n",
            "Iteration:  697 || Loss:  19.806295\n",
            "Iteration:  698 || Loss:  13.827847\n",
            "Iteration:  699 || Loss:  23.406078\n",
            "Iteration:  700 || Loss:  23.437006\n",
            "Iteration:  701 || Loss:  18.052938\n",
            "Iteration:  702 || Loss:  17.563574\n",
            "Iteration:  703 || Loss:  21.922985\n",
            "Iteration:  704 || Loss:  27.01393\n",
            "Iteration:  705 || Loss:  14.373086\n",
            "Iteration:  706 || Loss:  17.566317\n",
            "Iteration:  707 || Loss:  25.576862\n",
            "Iteration:  708 || Loss:  23.861769\n",
            "Iteration:  709 || Loss:  26.341154\n",
            "Iteration:  710 || Loss:  21.505657\n",
            "Iteration:  711 || Loss:  13.325846\n",
            "Iteration:  712 || Loss:  23.21492\n",
            "Iteration:  713 || Loss:  15.85201\n",
            "Iteration:  714 || Loss:  30.223\n",
            "Iteration:  715 || Loss:  17.19903\n",
            "Iteration:  716 || Loss:  17.758572\n",
            "Iteration:  717 || Loss:  15.2145405\n",
            "Iteration:  718 || Loss:  30.29009\n",
            "Iteration:  719 || Loss:  14.225088\n",
            "Iteration:  720 || Loss:  22.583912\n",
            "Iteration:  721 || Loss:  22.345032\n",
            "Iteration:  722 || Loss:  30.47895\n",
            "Iteration:  723 || Loss:  22.310232\n",
            "Iteration:  724 || Loss:  27.751541\n",
            "Iteration:  725 || Loss:  18.389341\n",
            "Iteration:  726 || Loss:  24.024857\n",
            "Iteration:  727 || Loss:  19.134037\n",
            "Iteration:  728 || Loss:  17.832327\n",
            "Iteration:  729 || Loss:  14.957832\n",
            "Iteration:  730 || Loss:  14.974697\n",
            "Iteration:  731 || Loss:  26.925625\n",
            "Iteration:  732 || Loss:  26.635687\n",
            "Iteration:  733 || Loss:  17.105112\n",
            "Iteration:  734 || Loss:  28.48621\n",
            "Iteration:  735 || Loss:  13.499226\n",
            "Iteration:  736 || Loss:  23.788473\n",
            "Iteration:  737 || Loss:  31.919228\n",
            "Iteration:  738 || Loss:  18.779047\n",
            "Iteration:  739 || Loss:  15.922272\n",
            "Iteration:  740 || Loss:  21.66311\n",
            "Iteration:  741 || Loss:  25.701002\n",
            "Iteration:  742 || Loss:  20.129377\n",
            "Iteration:  743 || Loss:  18.026575\n",
            "Iteration:  744 || Loss:  16.066507\n",
            "Iteration:  745 || Loss:  15.030142\n",
            "Iteration:  746 || Loss:  17.239956\n",
            "Iteration:  747 || Loss:  35.007828\n",
            "Iteration:  748 || Loss:  15.227844\n",
            "Iteration:  749 || Loss:  12.079559\n",
            "Iteration:  750 || Loss:  18.624596\n",
            "Iteration:  751 || Loss:  20.712824\n",
            "Iteration:  752 || Loss:  24.358343\n",
            "Iteration:  753 || Loss:  22.37023\n",
            "Iteration:  754 || Loss:  20.669176\n",
            "Iteration:  755 || Loss:  26.834133\n",
            "Iteration:  756 || Loss:  17.10784\n",
            "Iteration:  757 || Loss:  14.602873\n",
            "Iteration:  758 || Loss:  24.563923\n",
            "Iteration:  759 || Loss:  17.62393\n",
            "Iteration:  760 || Loss:  21.908531\n",
            "Iteration:  761 || Loss:  19.925632\n",
            "Iteration:  762 || Loss:  17.8962\n",
            "Iteration:  763 || Loss:  17.534964\n",
            "Iteration:  764 || Loss:  15.303896\n",
            "Iteration:  765 || Loss:  28.005066\n",
            "Iteration:  766 || Loss:  22.531307\n",
            "Iteration:  767 || Loss:  21.508383\n",
            "Iteration:  768 || Loss:  20.154694\n",
            "Iteration:  769 || Loss:  20.938034\n",
            "Iteration:  770 || Loss:  18.48551\n",
            "Iteration:  771 || Loss:  8.563377\n",
            "Iteration:  772 || Loss:  26.27978\n",
            "Iteration:  773 || Loss:  21.127235\n",
            "Iteration:  774 || Loss:  20.666954\n",
            "Iteration:  775 || Loss:  19.859247\n",
            "Iteration:  776 || Loss:  30.791594\n",
            "Iteration:  777 || Loss:  29.215044\n",
            "Iteration:  778 || Loss:  18.21759\n",
            "Iteration:  779 || Loss:  13.475772\n",
            "Iteration:  780 || Loss:  25.810495\n",
            "Iteration:  781 || Loss:  28.087267\n",
            "Iteration:  782 || Loss:  16.86141\n",
            "Iteration:  783 || Loss:  18.441422\n",
            "Iteration:  784 || Loss:  16.201904\n",
            "Iteration:  785 || Loss:  22.039694\n",
            "Iteration:  786 || Loss:  20.87682\n",
            "Iteration:  787 || Loss:  22.746626\n",
            "Iteration:  788 || Loss:  23.927574\n",
            "Iteration:  789 || Loss:  22.230873\n",
            "Iteration:  790 || Loss:  15.747972\n",
            "Iteration:  791 || Loss:  20.437408\n",
            "Iteration:  792 || Loss:  22.400557\n",
            "Iteration:  793 || Loss:  20.366661\n",
            "Iteration:  794 || Loss:  24.115507\n",
            "Iteration:  795 || Loss:  19.032185\n",
            "Iteration:  796 || Loss:  29.164845\n",
            "Iteration:  797 || Loss:  22.13675\n",
            "Iteration:  798 || Loss:  13.011129\n",
            "Iteration:  799 || Loss:  24.560188\n",
            "Iteration:  800 || Loss:  18.77176\n",
            "Iteration:  801 || Loss:  36.064884\n",
            "Iteration:  802 || Loss:  23.838375\n",
            "Iteration:  803 || Loss:  30.950386\n",
            "Iteration:  804 || Loss:  17.324322\n",
            "Iteration:  805 || Loss:  22.520576\n",
            "Iteration:  806 || Loss:  26.453758\n",
            "Iteration:  807 || Loss:  16.912437\n",
            "Iteration:  808 || Loss:  15.5108795\n",
            "Iteration:  809 || Loss:  20.545687\n",
            "Iteration:  810 || Loss:  24.445137\n",
            "Iteration:  811 || Loss:  14.228166\n",
            "Iteration:  812 || Loss:  25.385489\n",
            "Iteration:  813 || Loss:  24.661036\n",
            "Iteration:  814 || Loss:  17.67426\n",
            "Iteration:  815 || Loss:  24.98674\n",
            "Iteration:  816 || Loss:  18.681107\n",
            "Iteration:  817 || Loss:  25.528328\n",
            "Iteration:  818 || Loss:  22.4914\n",
            "Iteration:  819 || Loss:  28.90325\n",
            "Iteration:  820 || Loss:  15.703228\n",
            "Iteration:  821 || Loss:  30.469955\n",
            "Iteration:  822 || Loss:  34.65057\n",
            "Iteration:  823 || Loss:  30.746054\n",
            "Iteration:  824 || Loss:  17.63415\n",
            "Iteration:  825 || Loss:  27.412083\n",
            "Iteration:  826 || Loss:  19.55181\n",
            "Iteration:  827 || Loss:  24.847383\n",
            "Iteration:  828 || Loss:  19.034689\n",
            "Iteration:  829 || Loss:  6.4212894\n",
            "Iteration:  830 || Loss:  15.824461\n",
            "Iteration:  831 || Loss:  22.092958\n",
            "Iteration:  832 || Loss:  24.650866\n",
            "Iteration:  833 || Loss:  15.03526\n",
            "Iteration:  834 || Loss:  22.104397\n",
            "Iteration:  835 || Loss:  22.452791\n",
            "Iteration:  836 || Loss:  24.399956\n",
            "Iteration:  837 || Loss:  27.252842\n",
            "Iteration:  838 || Loss:  14.640966\n",
            "Iteration:  839 || Loss:  26.956045\n",
            "Iteration:  840 || Loss:  18.93561\n",
            "Iteration:  841 || Loss:  19.068592\n",
            "Iteration:  842 || Loss:  21.3278\n",
            "Iteration:  843 || Loss:  18.091965\n",
            "Iteration:  844 || Loss:  21.691051\n",
            "Iteration:  845 || Loss:  17.143406\n",
            "Iteration:  846 || Loss:  21.220564\n",
            "Iteration:  847 || Loss:  23.361298\n",
            "Iteration:  848 || Loss:  17.69709\n",
            "Iteration:  849 || Loss:  21.69469\n",
            "Iteration:  850 || Loss:  27.682224\n",
            "Iteration:  851 || Loss:  13.919728\n",
            "Iteration:  852 || Loss:  19.900524\n",
            "Iteration:  853 || Loss:  15.415119\n",
            "Iteration:  854 || Loss:  29.945766\n",
            "Iteration:  855 || Loss:  16.066702\n",
            "Iteration:  856 || Loss:  19.868326\n",
            "Iteration:  857 || Loss:  23.489967\n",
            "Iteration:  858 || Loss:  22.338982\n",
            "Iteration:  859 || Loss:  19.9939\n",
            "Iteration:  860 || Loss:  8.78816\n",
            "Iteration:  861 || Loss:  24.56448\n",
            "Iteration:  862 || Loss:  23.99545\n",
            "Iteration:  863 || Loss:  22.557787\n",
            "Iteration:  864 || Loss:  16.547983\n",
            "Iteration:  865 || Loss:  16.194067\n",
            "Iteration:  866 || Loss:  23.372599\n",
            "Iteration:  867 || Loss:  22.819046\n",
            "Iteration:  868 || Loss:  21.67684\n",
            "Iteration:  869 || Loss:  23.422056\n",
            "Iteration:  870 || Loss:  20.1196\n",
            "Iteration:  871 || Loss:  17.541441\n",
            "Iteration:  872 || Loss:  16.372505\n",
            "Iteration:  873 || Loss:  34.164852\n",
            "Iteration:  874 || Loss:  14.240482\n",
            "Iteration:  875 || Loss:  19.314037\n",
            "Iteration:  876 || Loss:  17.289173\n",
            "Iteration:  877 || Loss:  16.95848\n",
            "Iteration:  878 || Loss:  19.591503\n",
            "Iteration:  879 || Loss:  15.358062\n",
            "Iteration:  880 || Loss:  18.714622\n",
            "Iteration:  881 || Loss:  15.503229\n",
            "Iteration:  882 || Loss:  25.216255\n",
            "Iteration:  883 || Loss:  27.793814\n",
            "Iteration:  884 || Loss:  20.77711\n",
            "Iteration:  885 || Loss:  18.47757\n",
            "Iteration:  886 || Loss:  16.122337\n",
            "Iteration:  887 || Loss:  17.40115\n",
            "Iteration:  888 || Loss:  20.253311\n",
            "Iteration:  889 || Loss:  18.512608\n",
            "Iteration:  890 || Loss:  30.460745\n",
            "Iteration:  891 || Loss:  12.515015\n",
            "Iteration:  892 || Loss:  18.447676\n",
            "Iteration:  893 || Loss:  15.6156225\n",
            "Iteration:  894 || Loss:  23.71467\n",
            "Iteration:  895 || Loss:  13.956591\n",
            "Iteration:  896 || Loss:  20.425413\n",
            "Iteration:  897 || Loss:  11.840672\n",
            "Iteration:  898 || Loss:  12.813383\n",
            "Iteration:  899 || Loss:  12.981183\n",
            "Iteration:  900 || Loss:  19.026623\n",
            "Iteration:  901 || Loss:  17.175179\n",
            "Iteration:  902 || Loss:  25.36881\n",
            "Iteration:  903 || Loss:  16.069118\n",
            "Iteration:  904 || Loss:  25.426266\n",
            "Iteration:  905 || Loss:  26.906364\n",
            "Iteration:  906 || Loss:  27.645992\n",
            "Iteration:  907 || Loss:  26.036419\n",
            "Iteration:  908 || Loss:  29.683405\n",
            "Iteration:  909 || Loss:  26.878788\n",
            "Iteration:  910 || Loss:  18.059124\n",
            "Iteration:  911 || Loss:  21.350977\n",
            "Iteration:  912 || Loss:  25.365665\n",
            "Iteration:  913 || Loss:  24.066921\n",
            "Iteration:  914 || Loss:  33.36477\n",
            "Iteration:  915 || Loss:  13.133589\n",
            "Iteration:  916 || Loss:  19.581709\n",
            "Iteration:  917 || Loss:  18.738373\n",
            "Iteration:  918 || Loss:  14.615074\n",
            "Iteration:  919 || Loss:  26.92906\n",
            "Iteration:  920 || Loss:  18.301432\n",
            "Iteration:  921 || Loss:  15.6598425\n",
            "Iteration:  922 || Loss:  18.956543\n",
            "Iteration:  923 || Loss:  15.301256\n",
            "Iteration:  924 || Loss:  23.787582\n",
            "Iteration:  925 || Loss:  24.44941\n",
            "Iteration:  926 || Loss:  15.442863\n",
            "Iteration:  927 || Loss:  23.816963\n",
            "Iteration:  928 || Loss:  22.931204\n",
            "Iteration:  929 || Loss:  20.613836\n",
            "Iteration:  930 || Loss:  16.95146\n",
            "Iteration:  931 || Loss:  20.000246\n",
            "Iteration:  932 || Loss:  19.365028\n",
            "Iteration:  933 || Loss:  23.149834\n",
            "Iteration:  934 || Loss:  15.760986\n",
            "Iteration:  935 || Loss:  24.004223\n",
            "Iteration:  936 || Loss:  26.465675\n",
            "Iteration:  937 || Loss:  15.837619\n",
            "Iteration:  938 || Loss:  19.730354\n",
            "Iteration:  939 || Loss:  17.53004\n",
            "Iteration:  940 || Loss:  22.883287\n",
            "Iteration:  941 || Loss:  19.8052\n",
            "Iteration:  942 || Loss:  15.723119\n",
            "Iteration:  943 || Loss:  17.832855\n",
            "Iteration:  944 || Loss:  19.973478\n",
            "Iteration:  945 || Loss:  26.942238\n",
            "Iteration:  946 || Loss:  27.555656\n",
            "Iteration:  947 || Loss:  22.18086\n",
            "Iteration:  948 || Loss:  19.531387\n",
            "Iteration:  949 || Loss:  25.879631\n",
            "Iteration:  950 || Loss:  18.733335\n",
            "Iteration:  951 || Loss:  33.085598\n",
            "Iteration:  952 || Loss:  19.21042\n",
            "Iteration:  953 || Loss:  18.77913\n",
            "Iteration:  954 || Loss:  25.21525\n",
            "Iteration:  955 || Loss:  21.852493\n",
            "Iteration:  956 || Loss:  10.353948\n",
            "Iteration:  957 || Loss:  17.212212\n",
            "Iteration:  958 || Loss:  18.280773\n",
            "Iteration:  959 || Loss:  16.619295\n",
            "Iteration:  960 || Loss:  26.789743\n",
            "Iteration:  961 || Loss:  21.508923\n",
            "Iteration:  962 || Loss:  18.14509\n",
            "Iteration:  963 || Loss:  16.29078\n",
            "Iteration:  964 || Loss:  15.144958\n",
            "Iteration:  965 || Loss:  17.580158\n",
            "Iteration:  966 || Loss:  17.951366\n",
            "Iteration:  967 || Loss:  11.667335\n",
            "Iteration:  968 || Loss:  19.121668\n",
            "Iteration:  969 || Loss:  28.908226\n",
            "Iteration:  970 || Loss:  17.761265\n",
            "Iteration:  971 || Loss:  19.08647\n",
            "Iteration:  972 || Loss:  17.363148\n",
            "Iteration:  973 || Loss:  10.433818\n",
            "Iteration:  974 || Loss:  25.67443\n",
            "Iteration:  975 || Loss:  15.381017\n",
            "Iteration:  976 || Loss:  20.083635\n",
            "Iteration:  977 || Loss:  22.7777\n",
            "Iteration:  978 || Loss:  20.890123\n",
            "Iteration:  979 || Loss:  20.878773\n",
            "Iteration:  980 || Loss:  25.594902\n",
            "Iteration:  981 || Loss:  18.093985\n",
            "Iteration:  982 || Loss:  14.749368\n",
            "Iteration:  983 || Loss:  9.620378\n",
            "Iteration:  984 || Loss:  14.3795395\n",
            "Iteration:  985 || Loss:  17.786888\n",
            "Iteration:  986 || Loss:  15.91879\n",
            "Iteration:  987 || Loss:  24.82812\n",
            "Iteration:  988 || Loss:  34.7146\n",
            "Iteration:  989 || Loss:  18.13568\n",
            "Iteration:  990 || Loss:  16.482117\n",
            "Iteration:  991 || Loss:  24.904154\n",
            "Iteration:  992 || Loss:  19.24353\n",
            "Iteration:  993 || Loss:  11.770171\n",
            "Iteration:  994 || Loss:  24.479277\n",
            "Iteration:  995 || Loss:  18.290876\n",
            "Iteration:  996 || Loss:  12.952024\n",
            "Iteration:  997 || Loss:  20.410118\n",
            "Iteration:  998 || Loss:  19.204096\n",
            "Iteration:  999 || Loss:  18.856827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyDRkr7rUxGW",
        "colab_type": "text"
      },
      "source": [
        "####Visualizing loss of the trained model over the iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUoaeGs1hy6-",
        "colab_type": "code",
        "outputId": "73834afd-88cc-4d2a-9e86-7545e3453dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(combined_loss)\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.xlabel('Iteration')\n",
        "plt.show()"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smKeLsFSU_tX",
        "colab_type": "text"
      },
      "source": [
        "Figure above shows that the loss of the model that uses a given learning rate ($\\alpha$ = 0.001) in the Stochastic Gradient Descent (SGD) optimizer can reduce the loss along the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeL8PW_mWbAv",
        "colab_type": "text"
      },
      "source": [
        "####Calculating the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJHY3fi0ehGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "true_pred = tf.cast(correct_prediction, \"float\")\n",
        "accuracy = tf.reduce_mean(true_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpOMvCv4equv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_prediction, true_pred, accuracy = sess.run([correct_prediction, true_pred, accuracy], \n",
        "                                                   feed_dict = {x: mnist.test.images, y_: mnist.test.labels})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQi6laj5e3n0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fa2aad5-b833-4bce-9654-ad25b54f1e36"
      },
      "source": [
        "correct_prediction.tolist()[:10]"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True, True, True, True, True, True, True, True, False, True]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2zuHthUWjqF",
        "colab_type": "text"
      },
      "source": [
        "Function tf.equal() to get the correct_prediction returns boolean result, where True and False indicate the right and wrong class prediction respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tteBvJDTjGT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2967d09-4312-42b9-efe6-2ad2d82790c5"
      },
      "source": [
        "true_pred.tolist()[:10]"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1DsqQa7XgcO",
        "colab_type": "text"
      },
      "source": [
        "Function tf.cast() transforms the result of correct_prediction to the certain destination type of variable (in this case, boolean results are transformed to float, where 1=True and 0=False)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXrXH0JqT0uW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75eac6a2-c463-4c88-a50b-02112e741835"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJcXBy6TZLK-",
        "colab_type": "text"
      },
      "source": [
        "Calculating the mean of binary output from true_pred can directly results the accuracy of out trained model. Applying tf.reduce_mean() here is the same as counting the instances with the true prediction divided by the number of all instances used for training the model."
      ]
    }
  ]
}